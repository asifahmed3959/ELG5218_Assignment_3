{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c58ad8",
   "metadata": {},
   "source": [
    "Answer:1.a) \n",
    "In the link, they are trying to compute linear regression with $y_i = f(X_i) + \\epsilon_i, \\quad for \\quad i=1,...n$ . Where $f(x_i)$ will be computed using gaussian process $f(x)\\sim GP(m(x), k(x,\\cdot ))$, and here, $m(x)$ is the mean which will be considered zero, and k is the kernel (RBFspecificially).  A fully Bayesian model for the nonlinear data above can be specified as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "   y | f \\sim MvNormal(f,\\sigma^2I)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "   f|\\alpha,\\rho \\sim MvNormal(0_N, K_{\\alpha, \\rho})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "   \\alpha \\sim LogNormal(0, 0.1)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "   \\rho \\sim LogNormal(0, 1)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "   \\sigma \\sim LogNormal(0, 1)\n",
    "\\end{equation}\n",
    "\n",
    "As they are considering mean zero for the GP, we to figure out three parameters which are: $\\alpha$,$\\rho$, $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb1633",
   "metadata": {},
   "source": [
    "Answer:1.b)\n",
    "\n",
    "Gaussian Process ia non-parametric method which means, the number of parameters are infinite. Moreover, for obtaining relationship and function, it is computer through methods: $f(x) = \\sum_{i=1}^{N} w_i(x)y_i$ where $w_i(x) = \\frac{k(x,x_i)}{\\sum_{i=1}^{N}k(x,x_i')}$ which means observations that are close to x have a higher weight than observation that are further away. Likewise, weight computed from x and observed x_i with a kernel k. For this method to proceed all training data have to be processed for prediction and are therefore slower at inference time than parametric method. For each data points, the kernel has to be computed. Therefore, as dataset increases so increases complexity. Matrix multiplication is indeed of complexity $O(n^{3})$. For the kernel calculation saw we are using $RBF= \\sigma_{f}^{2}exp\\left ( \\frac{-1}{2l^2} (x_i-x_j)^T(x_i-x_j) \\right )$. Therefore, we are transposing and calculating matrix multiplication whose complexity is $O(n^3)$. Therefore, for GP the computational complexity is $O(n^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52666b",
   "metadata": {},
   "source": [
    "Answer:1. c) 1) - d\n",
    "2) - a\n",
    "3) - e\n",
    "4) - b\n",
    "5) - c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb31ef3",
   "metadata": {},
   "source": [
    "Answer:3.a)\n",
    "\n",
    "AIC uses a model's maximum likelihood estimation (log-likelihood) as a measure of it. Log-likelihood is a measure of how likely one is to see their observed data, given a model. The model with maximum likelihood is the one that \"fits\" the data the best.\n",
    "\n",
    "As per the definition of AIC is $AIC = 2K - 2ln(\\hat{l})$ where $\\hat{l}$ is the likelihood of model & k is the number of parameters. When comparing a simple model to a complex model, the log likelihood of the complex model must be greater than the log likelihood of the simple model by atleast the number of additional parameter for the AIC to go down, indicating that the more complex model is a better fit. \n",
    "\n",
    "For DIC: $log P(x|\\hat{\\theta}_{map}) - 2V_{j=1}log P(x|\\theta_{j})$ where the value of $\\theta$ that maximizes the posterior density. The idea here is that if we have more uncertainity in our parameter values then there will be a lot of uncertainty in the fit, which is measured by the variance term here. Thus more uncertainty in each parameter values indicates too commplex model, given the current cirscumstance. In other words, overfitting the model. \n",
    "\n",
    "FOR WAIC: $\\sum_{i=1}^{n} {log \\frac{1}{s}(\\sum_{j=1}^{s} P(x_i|\\theta_j))}$ $- \\sum_{i=1}^{n} V_{j=1}^s log P(X_j|\\theta_j)$ \n",
    "\n",
    "As it is taking the sum of all datapoints in our samples from i=1 to i=n of the log of the average value of the likelihood of $P(x_i|\\theta_j)$, which essentially measures the average fit of the model across all our posterior uncertainty, where we penalize point wise, summing from i=1 to i=n the variance of all our posterior draws of log of the probability, $log P(X_i|\\theta_j)$, which points out the best fir for the model without complicating it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9a91f",
   "metadata": {},
   "source": [
    "Answer:3.b) \n",
    "\n",
    "Model selection refers to methods where a single distribution function is chosen based on prior knowledge or by means of selection criteria. Model averaging refers to methods where the results of multiple distribution functions are combined. Suppose if we have K models we can select the best model or we can average them together or fuse them. Averaging a model will reduce the uncertainity in our predictions, while using a single model can lead to uncercainities which cannot be avoided. It is common practice in any form of modelling or statistical analysis to consider a range of models as possible representations of the observed reality. A single model is usually selected based on different criteria, such as (a) goodness-of-fit statistics, e.g. by using the chi-squared (χ2) test; (b) prior selection of a distribution function as a result of what Chamberlain (1965) referred to as “parental affection” towards a given model; or (c) standardization, such as the information criterion. In few fields of measurement, the selection of a single best distribution function represents an implicit assumption that the selected model can adequately describe the frequency of observed and future measurement, including the extreme ones. This assumption departs from the understanding that low and ordinary measurements (which might make up the significant record) are dominated by different processes compared to extreme measurements, which are often the main focus in predictions. Therefore, the selection of a single distribution model, which is valid for the whole range of data, may lead to uncertainty in the design estimates. Also, smaller measurements are known to influence the smoothing and extrapolation of the largest discharges in the record and in turn may lead to uncertain estimates of the design. Therefore, we will select the model which best represents our overall data and reduce uncertainty, we can do all sorts of hypothesis testing, and see if the selection criterion results in similar results for all our models which can definitely choose Model averaging over model selection, however, if a significant result is produced by model selection criteria, we can definitely choose model selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3157df",
   "metadata": {},
   "source": [
    "Answer:3.c)\n",
    "\n",
    "Say we would like to predict future values of y, which is a regression model, given some new data x, given previous training data that we have. Therefore, break it into k models. $P(y*|x*,D) = \\sum_{i=1}^{k} P(y*|x*, D, M_k) P(M_k|D)$ where $P(M_k|D)$ is weight. \n",
    "\n",
    "This weight factor can be calculated in two ways: \n",
    "One is using Information Criteria $W_k=\\frac{e^{\\frac{-1}{2}dIC_k}}{\\sum_{k=1}^{k} e^{\\frac{-1}{2}dIC_k}}$ where $dIC_k=WAIC_k-WAIC_{min}$. Basically, using an information criteria, which could be WAIC, and find the difference between WAIC for the modek k and the model with the minimum WAIC. \n",
    "\n",
    "\n",
    "Another way is to use stacking a model averages. Stacking is a direct approach for averaging point estimates from multiple models. In supervised learning, where the data are $((xi , yi ), i = 1, . . . , n )$ and each model M_k has a parametric form $\\hat{y}_k = f_k (x|θ_k )$, stacking is done in two steps (Ting and Witten, 1999). First, each model is fitted separately and the leave-one-out (LOO) predictor $\\hat{f}^{(-1)}_k (x_i) = E(y_i|\\hat{\\theta}_{k,y_i},M_k) $ is obtained for each model k and each data point i. In the second step, a weight for each model is obtained by minimizing the mean squared error, treating the leave-out-out predictors from the previous stage as covariates:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "        \\hat{w} = arg\\underset{w}{min} \\sum_{i=1}^{n}\\left( y_i - \\sum_{k}w_k\\hat{f}_{k}^{(-1)}(x_i))\\right)^2 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009413e",
   "metadata": {},
   "source": [
    "Either a positive constraint $w_k ≥ 0, k = 1, . . . K$, or a simplex constraint: $w_k ≥ 0, \\sum_{k}^{K} w_k=1$ guarantees a solution. Finally, the point prediction for a new data point with feature vector  ̃x is\n",
    "\n",
    "\\begin{equation}\n",
    "   \\tilde{y} = \\sum_{k=1}^{K} w_kf_k(\\tilde{x}|\\hat{\\theta}_{k,y1:n})\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
